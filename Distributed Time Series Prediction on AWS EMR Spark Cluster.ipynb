{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\"conf\": \n",
    " { \n",
    "     \"spark.pyspark.virtualenv.enabled\": \"false\",\n",
    "     \"spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_TYPE\":\"docker\",\n",
    "     \"spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_CLIENT_CONFIG\":\"hdfs:///user/hadoop/config.json\",\n",
    "     \"spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE\":\"574404580792.dkr.ecr.us-east-1.amazonaws.com/scalableml:s3spark\",\n",
    "     \"spark.executor.instances\":\"2\",\n",
    "     \"spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_CLIENT_CONFIG\":\"hdfs:///user/hadoop/config.json\",\n",
    "     \"spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE\":\"574404580792.dkr.ecr.us-east-1.amazonaws.com/scalableml:s3spark\"\n",
    " }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1d00f0fe704d65afa3a23efc039898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1623599779960_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-194.ec2.internal:20888/proxy/application_1623599779960_0002/\" class=\"emr-proxy-link\" emr-resource=\"j-5MFSJOFMORAL\n",
       "\" application-id=\"application_1623599779960_0002\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-92-28.ec2.internal:8042/node/containerlogs/container_1623599779960_0002_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144410aa16b1415ebc2c4d0ec20156ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = '****************'\n",
    "aws_secret = '*******************'\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "                        aws_secret_access_key=aws_secret)\n",
    "\n",
    "csv_obj = client.get_object(Bucket='spark-anand', Key='forecasted_1.csv')\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Humidity for Multiple Weather Parameters (Time Series Data)\n",
    "\n",
    "The daily temperate, wind, rainfall and humidity of a location is recorded and noted in the input dataset from 1990~2020s. \n",
    "\n",
    "Build a time series model to predict the humidity in Y2021, given these features: temperate, wind, rainfall. 2020Q4 data can be used as test dataset, while the remaining for training the model. To verify the model, you can use any metric to compare prediction and actual humidity values in 2020 Q4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c739c3434546ea8ee2b20aa5bdecbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import boto3\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\", font_scale=1.3)\n",
    "sns.set_style('white')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from time import time\n",
    "import matplotlib.ticker as tkr\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the statistics module\n",
    "from statistics import mean\n",
    "from statistics import median\n",
    "\n",
    "# To load the input data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# import tensorflow as tf \n",
    "# tf.test.gpu_device_name() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832537c57c58489caa05ab4d8320395b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date    temp    wind  rainfall  humidity\n",
      "0  19900101 28.4000 17.9600   20.4000   32.1800\n",
      "1  19900102 35.5000  2.2300    0.0000   22.8400\n",
      "2  19900103 17.4000  9.0600    0.0000   29.3800\n",
      "3  19900104 28.4000  1.5700    0.0000   26.3000\n",
      "4  19900105 28.3000  0.3000    0.0000   26.7500"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################\n",
    "# get your credentials from environment variables\n",
    "aws_id = '***************'\n",
    "aws_secret = '*********************'\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "                        aws_secret_access_key=aws_secret)\n",
    "\n",
    "\n",
    "def openFile(fileName):\n",
    "        csv_obj = client.get_object(Bucket='spark-anand', Key=fileName)\n",
    "        body = csv_obj['Body']\n",
    "        csv_string = body.read().decode('utf-8')\n",
    "\n",
    "        df = pd.read_csv(StringIO(csv_string))\n",
    "        return df\n",
    "######################################################\n",
    "                                    \n",
    "# Read the s&p 500 input data set and sorting based on date.\n",
    "observed = openFile(\"observed_1.csv\")\n",
    "observed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abc34da736442c1b4184bee77d04c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   week    temp    wind  rainfall  day  humScale  humidity\n",
      "0     1 28.4000 17.9600   20.4000    1  579.3600   32.1800\n",
      "1     1 35.5000  2.2300    0.0000    2    0.0000   22.8400\n",
      "2     1 17.4000  9.0600    0.0000    3    0.0000   29.3800\n",
      "3     1 28.4000  1.5700    0.0000    4    0.0000   26.3000\n",
      "4     1 28.3000  0.3000    0.0000    5    0.0000   26.7500\n",
      "5     1 35.4000 14.0400    0.0000    6    0.0000   20.7900\n",
      "6     1 11.4000  5.6700    0.0000    7    0.0000   32.9600\n",
      "7     2 29.5000 18.3600    0.0000    8    0.0000   21.8500\n",
      "8     2 12.8000 19.2400    0.0000    9    0.0000   29.5500\n",
      "9     2 32.2000  9.2700    0.0000   10    0.0000   22.7400"
     ]
    }
   ],
   "source": [
    "observed_filtered = observed[observed['temp'] < 100] \n",
    "len(observed_filtered)\n",
    "\n",
    "from datetime import datetime\n",
    "day_of_year = datetime.now().timetuple().tm_yday\n",
    "day_of_year\n",
    "\n",
    "\n",
    "# datetime.now()\n",
    "date_time_obj = datetime.strptime(\"040205\", '%y%m%d')\n",
    "date_time_obj.timetuple().tm_yday\n",
    "\n",
    "\n",
    "def convert_to_date(date):\n",
    "    date_as_string = str(date)\n",
    "    date_as_string = date_as_string[2:]\n",
    "    date_time_obj = datetime.strptime(date_as_string, '%y%m%d')\n",
    "    return date_time_obj\n",
    "    \n",
    "def convert_to_daynum(date):\n",
    "    date_as_string = str(date)\n",
    "    date_as_string = date_as_string[2:]\n",
    "    date_time_obj = datetime.strptime(date_as_string, '%y%m%d')\n",
    "\n",
    "    return date_time_obj.timetuple().tm_yday\n",
    "\n",
    "#     return int(year_as_string)\n",
    "\n",
    "observed_filtered['day'] = observed_filtered['date'].apply(convert_to_daynum)\n",
    "observed_filtered['date'] = observed_filtered['date'].apply(convert_to_date)\n",
    "\n",
    "observed_filtered.head()\n",
    "\n",
    "\n",
    "df = observed_filtered\n",
    "\n",
    "# df['date']=pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "df['week'] = df['date'].apply(lambda x: x.week)\n",
    "df['month'] = df['date'].apply(lambda x: x.month)\n",
    "df['dayinmonth'] = df['date'].apply(lambda x: x.day)\n",
    "df['humScale'] = df['temp'] * df['rainfall'] \n",
    "\n",
    "df.sort_values('date', inplace=True, ascending=True)\n",
    "# df=df.loc[:,['date','year','quarter','month','dayinmonth', 'temp', 'wind', 'rainfall', 'day', 'humidity']]\n",
    "df=df.loc[:,['week', 'temp', 'wind', 'rainfall', 'day', 'humScale', 'humidity']]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e01b47b9f4dfc9f2e17c857e2462c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11127"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visualization Module - Humidity\n",
    "\n",
    "humidity = df['humidity']\n",
    "x = np.arange(0, len(humidity))\n",
    "print(len(humidity))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, humidity, label=\"Humidity\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Humidity vs Time\", fontsize=20)\n",
    "plt.ylabel(\"Humidity\", fontsize=15)\n",
    "plt.xlabel(\"Time\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf755eec3c54c9f8e7ac7e1f421f9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from time import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "if not 'sc' in globals():\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "# Assemble all the features with VectorAssembler\n",
    "def assembleFeatures(dfRDD):\n",
    "\n",
    "    required_features = ['week',\n",
    "                        'temp',\n",
    "                        'wind',\n",
    "                        'rainfall',\n",
    "                        'day',\n",
    "                        'humScale'\n",
    "                       ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "    transformed_data = assembler.transform(dfRDD)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "\n",
    "def trainRFmodel(trainingData):\n",
    "\n",
    "    # Train a RandomForest model.\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "\n",
    "    # Chain indexer and forest in a Pipeline\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "    # Train model.  This also runs the indexer.\n",
    "    model = pipeline.fit(trainingData)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def testRFmodel(model, testData, isLabelled):\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    if isLabelled:\n",
    "        predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "        \n",
    "        # Select (prediction, true label) and compute test error\n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "    else:\n",
    "        predictions.select(\"prediction\", \"features\").show(5)\n",
    "\n",
    "    rfModel = model.stages[0]\n",
    "    print(rfModel)  # summary only\n",
    "    \n",
    "    return predictions.select(\"prediction\", \"features\")\n",
    "\n",
    "\n",
    "\n",
    "df.columns = ['week', 'temp', 'wind', 'rainfall', 'day', 'humScale', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d533cd12b18f4dc0a843416ac3d3881b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dfRDD = spark.createDataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412bee0d745f49db8a02563e46b14659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(week=1, temp=28.4, wind=17.96, rainfall=20.4, day=1, humScale=579.3599999999999, label=32.18)"
     ]
    }
   ],
   "source": [
    "dfRDD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f16fd8dd164d67b97242298ff5b0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+--------+---+------------------+-----+\n",
      "|week|temp| wind|rainfall|day|          humScale|label|\n",
      "+----+----+-----+--------+---+------------------+-----+\n",
      "|   1|28.4|17.96|    20.4|  1| 579.3599999999999|32.18|\n",
      "|   1|35.5| 2.23|     0.0|  2|               0.0|22.84|\n",
      "|   1|17.4| 9.06|     0.0|  3|               0.0|29.38|\n",
      "|   1|28.4| 1.57|     0.0|  4|               0.0| 26.3|\n",
      "|   1|28.3|  0.3|     0.0|  5|               0.0|26.75|\n",
      "|   1|35.4|14.04|     0.0|  6|               0.0|20.79|\n",
      "|   1|11.4| 5.67|     0.0|  7|               0.0|32.96|\n",
      "|   2|29.5|18.36|     0.0|  8|               0.0|21.85|\n",
      "|   2|12.8|19.24|     0.0|  9|               0.0|29.55|\n",
      "|   2|32.2| 9.27|     0.0| 10|               0.0|22.74|\n",
      "|   2|27.9|16.92|     0.0| 11|               0.0|23.38|\n",
      "|   2|13.8|10.96|    27.7| 12|            382.26|44.16|\n",
      "|   2|25.3|13.89|    57.6| 13|           1457.28|52.14|\n",
      "|   2|39.8|19.28|    52.4| 14|           2085.52|42.59|\n",
      "|   3|28.1| 4.26|    26.9| 15|            755.89|39.23|\n",
      "|   3|31.2|19.72|     0.0| 16|               0.0|20.88|\n",
      "|   3|19.8|  5.1|    33.5| 17| 663.3000000000001|45.14|\n",
      "|   3|38.2| 6.63|    41.2| 19|1573.8400000000001|40.34|\n",
      "|   3|32.8| 6.67|     0.0| 20|               0.0|23.55|\n",
      "|   4|30.8| 1.26|     0.0| 22|               0.0|24.96|\n",
      "+----+----+-----+--------+---+------------------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "dfRDD.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = assembleFeatures(dfRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fa61a294e34a8c9a5dadc4903feeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3000000), ('b', 5000000)]\n",
      "Time taken to run: 3.694195032119751 seconds"
     ]
    }
   ],
   "source": [
    "# Simple reduceByKey example in python\n",
    "# creating PairRDD x with key value pairs\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"a\", 1),\n",
    "                    (\"b\", 1), (\"b\", 1), (\"b\", 1), (\"b\", 1)]*1000000, 8)\n",
    " \n",
    "# Applying reduceByKey operation on x\n",
    "y = x.reduceByKey(lambda accum, n: accum + n)\n",
    "print(y.collect())\n",
    "\n",
    "print(f'Time taken to run: {time() - start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8b30d4876f43518eb4c8c3d26b9a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='a', _2=1), Row(_1='b', _2=1), Row(_1='a', _2=1), Row(_1='a', _2=1), Row(_1='b', _2=1)]"
     ]
    }
   ],
   "source": [
    "dfRDD = spark.createDataFrame(x)\n",
    "dfRDD.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#df.columns = ['week', 'temp', 'wind', 'rainfall', 'day', 'humScale', 'label']\n",
    "\n",
    "#dfRDD = spark.createDataFrame(df)\n",
    "\n",
    "transformed_data = assembleFeatures(dfRDD)\n",
    "transformed_data.show()\n",
    "\n",
    "# Split the data\n",
    "(trainingData, testData) = transformed_data.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6bd252c39c4be7b0191e7b698ae461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+--------+---+------------------+-----+--------------------+\n",
      "|week|temp| wind|rainfall|day|          humScale|label|            features|\n",
      "+----+----+-----+--------+---+------------------+-----+--------------------+\n",
      "|   1|28.4|17.96|    20.4|  1| 579.3599999999999|32.18|[1.0,28.4,17.96,2...|\n",
      "|   1|35.5| 2.23|     0.0|  2|               0.0|22.84|[1.0,35.5,2.23,0....|\n",
      "|   1|17.4| 9.06|     0.0|  3|               0.0|29.38|[1.0,17.4,9.06,0....|\n",
      "|   1|28.4| 1.57|     0.0|  4|               0.0| 26.3|[1.0,28.4,1.57,0....|\n",
      "|   1|28.3|  0.3|     0.0|  5|               0.0|26.75|[1.0,28.3,0.3,0.0...|\n",
      "|   1|35.4|14.04|     0.0|  6|               0.0|20.79|[1.0,35.4,14.04,0...|\n",
      "|   1|11.4| 5.67|     0.0|  7|               0.0|32.96|[1.0,11.4,5.67,0....|\n",
      "|   2|29.5|18.36|     0.0|  8|               0.0|21.85|[2.0,29.5,18.36,0...|\n",
      "|   2|12.8|19.24|     0.0|  9|               0.0|29.55|[2.0,12.8,19.24,0...|\n",
      "|   2|32.2| 9.27|     0.0| 10|               0.0|22.74|[2.0,32.2,9.27,0....|\n",
      "|   2|27.9|16.92|     0.0| 11|               0.0|23.38|[2.0,27.9,16.92,0...|\n",
      "|   2|13.8|10.96|    27.7| 12|            382.26|44.16|[2.0,13.8,10.96,2...|\n",
      "|   2|25.3|13.89|    57.6| 13|           1457.28|52.14|[2.0,25.3,13.89,5...|\n",
      "|   2|39.8|19.28|    52.4| 14|           2085.52|42.59|[2.0,39.8,19.28,5...|\n",
      "|   3|28.1| 4.26|    26.9| 15|            755.89|39.23|[3.0,28.1,4.26,26...|\n",
      "|   3|31.2|19.72|     0.0| 16|               0.0|20.88|[3.0,31.2,19.72,0...|\n",
      "|   3|19.8|  5.1|    33.5| 17| 663.3000000000001|45.14|[3.0,19.8,5.1,33....|\n",
      "|   3|38.2| 6.63|    41.2| 19|1573.8400000000001|40.34|[3.0,38.2,6.63,41...|\n",
      "|   3|32.8| 6.67|     0.0| 20|               0.0|23.55|[3.0,32.8,6.67,0....|\n",
      "|   4|30.8| 1.26|     0.0| 22|               0.0|24.96|[4.0,30.8,1.26,0....|\n",
      "+----+----+-----+--------+---+------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken to run: 0.5975096225738525 seconds\n",
      "+------------------+-----+--------------------+\n",
      "|        prediction|label|            features|\n",
      "+------------------+-----+--------------------+\n",
      "| 23.16851461048668|20.79|[1.0,35.4,14.04,0...|\n",
      "|29.757458136269626|29.34|[2.0,16.9,9.22,0....|\n",
      "|28.444565123310195|26.85|[2.0,23.2,10.37,0...|\n",
      "| 28.68571366837302|28.76|[3.0,23.6,2.51,0....|\n",
      "| 45.00868659690367|41.55|[4.0,20.5,12.18,2...|\n",
      "+------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 2.86237\n",
      "RandomForestRegressionModel: uid=RandomForestRegressor_ec204607f06e, numTrees=20, numFeatures=6\n",
      "DataFrame[prediction: double, features: vector]"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from time import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "\n",
    "if not 'sc' in globals():\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "# Assemble all the features with VectorAssembler\n",
    "def assembleFeatures(dfRDD):\n",
    "\n",
    "    required_features = ['week',\n",
    "                        'temp',\n",
    "                        'wind',\n",
    "                        'rainfall',\n",
    "                        'day',\n",
    "                        'humScale'\n",
    "                       ]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
    "    transformed_data = assembler.transform(dfRDD)\n",
    "    \n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "\n",
    "def trainRFmodel(trainingData):\n",
    "\n",
    "    # Train a RandomForest model.\n",
    "    rf = RandomForestRegressor(featuresCol=\"features\")\n",
    "\n",
    "    # Chain indexer and forest in a Pipeline\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "    # Train model.  This also runs the indexer.\n",
    "    model = pipeline.fit(trainingData)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def testRFmodel(model, testData, isLabelled):\n",
    "\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Select example rows to display.\n",
    "    if isLabelled:\n",
    "        predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "        \n",
    "        # Select (prediction, true label) and compute test error\n",
    "        evaluator = RegressionEvaluator(\n",
    "            labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "    else:\n",
    "        predictions.select(\"prediction\", \"features\").show(5)\n",
    "\n",
    "    rfModel = model.stages[0]\n",
    "    print(rfModel)  # summary only\n",
    "    \n",
    "    return predictions.select(\"prediction\", \"features\")\n",
    "\n",
    "\n",
    "\n",
    "df.columns = ['week', 'temp', 'wind', 'rainfall', 'day', 'humScale', 'label']\n",
    "\n",
    "dfRDD = spark.createDataFrame(df)\n",
    "\n",
    "transformed_data = assembleFeatures(dfRDD)\n",
    "transformed_data.show()\n",
    "\n",
    "# Split the data\n",
    "(trainingData, testData) = transformed_data.randomSplit([0.9,0.1])\n",
    "\n",
    "start = time()\n",
    "model = trainRFmodel(trainingData)\n",
    "print(f'Time taken to run: {time() - start} seconds')\n",
    "\n",
    "testRFmodel(model, testData, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the s&p 500 input data set and sorting based on date.\n",
    "observed_2020 = openFile(\"test_input_1.csv\")\n",
    "observed_2020.head()\n",
    "\n",
    "\n",
    "observed_2020['day'] = observed_2020['date'].apply(convert_to_daynum)\n",
    "observed_2020['date'] = observed_2020['date'].apply(convert_to_date)\n",
    "\n",
    "\n",
    "df = observed_2020\n",
    "\n",
    "# df['date']=pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "df['week'] = df['date'].apply(lambda x: x.week)\n",
    "df['month'] = df['date'].apply(lambda x: x.month)\n",
    "df['dayinmonth'] = df['date'].apply(lambda x: x.day)\n",
    "\n",
    "df['humScale'] = df['temp'] * df['rainfall']\n",
    "\n",
    "df.sort_values('date', inplace=True, ascending=True)\n",
    "# df=df.loc[:,['date','year','quarter','month','dayinmonth', 'temp', 'wind', 'rainfall', 'day', 'humidity']]\n",
    "df=df.loc[:,['week', 'temp', 'wind', 'rainfall', 'day', 'humScale']]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()\n",
    "\n",
    "dfRDD_2020 = spark.createDataFrame(df)\n",
    "\n",
    "transformed_data_2020 = assembleFeatures(dfRDD_2020)\n",
    "transformed_data_2020.show()\n",
    "\n",
    "# Split the data\n",
    "# (trainingData, testData) = transformed_data_2020.randomSplit([0.9,0.1])\n",
    "\n",
    "\n",
    "predictions = testRFmodel(model, transformed_data_2020, False)\n",
    "predictions.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the input data set to get actual humidity values to compare with prediction\n",
    "output_2020 = openFile(\"test_output_1.csv\")\n",
    "output_2020.head(10)\n",
    "\n",
    "output_2020['humidity'].values\n",
    "\n",
    "# len(np.array(predictions.toPandas()['prediction']))\n",
    "actual_humidity = output_2020['humidity'].values\n",
    "mean_squared_error(actual_humidity, np.array(predictions.toPandas()['prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
